% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}

\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\floatname{literal-block}{Listing }



\title{Turing Neural Networks Documentation}
\date{October 14, 2015}
\release{0.1}
\author{Giovanni Sirio Carmantini}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{License}
\label{license:turing-neural-networks-code-documentation}\label{license::doc}\label{license:license}
The MIT License (MIT)

Copyright (c) \textless{}2015\textgreater{} \textless{}Giovanni Sirio Carmantini\textgreater{}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ``Software''), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


\chapter{Introduction}
\label{introduction_docs:introduction}\label{introduction_docs::doc}
This files documents the code used in the paper ``From Turing Machines
to Neural Networks'', implementing the presented mapping. In the
example included with the code it is possible to observe how to create
a Neural Network simulating the computation of any arbitrary Turing
Machine given the Machine description.

For the most up-to-date version of the code and documentation please
visit the git repository at
\href{https://github.com/TuringApproved/Turing\_Neural\_Networks}{https://github.com/TuringApproved/Turing\_Neural\_Networks}.

For installation instructions, follow the README.txt included with the
code.


\chapter{symdyn module documentation}
\label{symdyn_docs::doc}\label{symdyn_docs:symdyn-module-documentation}
The symdyn module contains various classes implementing objects in representation theory, such as GÃ¶del encoders and Generalized Shifts, needed to construct Nonlinear Dynamical Automata  {[}Moore91{]}.


\section{Code}
\label{symdyn_docs:code}\label{symdyn_docs:module-symdyn}\index{symdyn (module)}\index{as\_list() (in module symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.as_list}\pysiglinewithargsret{\code{symdyn.}\bfcode{as\_list}}{\emph{arg}}{}
Convenience function used to make sure that a sequence is always
represented by a list of symbols. Sometimes a single-symbol
sequence is passed in the form of a string representing the
symbol. This has to be converted to a one-item list for
consistency, as a list of symbols is what many of the following
functions expect.

\end{fulllineitems}

\index{FractalEncoder (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.FractalEncoder}\pysigline{\strong{class }\code{symdyn.}\bfcode{FractalEncoder}}
Abstract class implementing a Fractal Encoder (generalizing the
Godel Encoders).

\end{fulllineitems}

\index{GodelEncoder (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.GodelEncoder}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{GodelEncoder}}{\emph{alphabet}}{}
Create a Godel Encoding given an alphabet. Given the encoding, you can
then encode strings as simple numbers or cylinder sets (defined by their
upper and lower bounds)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{alphabet}} -- a list of symbols. The encoder will first associate
one number to each symbol. In particular the symbol in alphabet{[}0{]}
will be associated with number 0, and so on.

\end{description}\end{quote}
\index{encode\_sequence() (symdyn.GodelEncoder method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.GodelEncoder.encode_sequence}\pysiglinewithargsret{\bfcode{encode\_sequence}}{\emph{sequence}}{}
Return Godel encoding of a sequence (passed as a list of strings,
each one representing a symbol, or a string in case of a single
symbol).

\end{fulllineitems}

\index{encode\_cylinder() (symdyn.GodelEncoder method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.GodelEncoder.encode_cylinder}\pysiglinewithargsret{\bfcode{encode\_cylinder}}{\emph{sequence}, \emph{rescale=False}}{}
Return Godel encoding of cylinder set of a sequence (passed as a
list of strings, each one representing a symbol, or a string
in case of a single symbol).

If rescale=True the values are no
longer bound to \([0,1]\), and the most significant digit
of the returned values in base \(g\), where \(g\) is
the number of symbols in the alphabet, is equal to the Godel
encoding of the first symbol in the string.

\end{fulllineitems}


\end{fulllineitems}

\index{compactGodelEncoder (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.compactGodelEncoder}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{compactGodelEncoder}}{\emph{ge\_q}, \emph{ge\_s}}{}
Create Godel Encoder as described in our submitted paper.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{ge\_q}} -- A Fractal Encoder for the simulated TM states.

\item {} 
\textbf{\texttt{ge\_s}} -- A Fractal Encoder for the simulated TM tape symbols.

\end{itemize}

\end{description}\end{quote}
\index{encode\_sequence() (symdyn.compactGodelEncoder method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.compactGodelEncoder.encode_sequence}\pysiglinewithargsret{\bfcode{encode\_sequence}}{\emph{sequence}}{}
Return Godel encoding of a sequence (passed as a list of strings,
each one representing a symbol, or a string in case of a single
symbol).

\end{fulllineitems}

\index{encode\_cylinder() (symdyn.compactGodelEncoder method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.compactGodelEncoder.encode_cylinder}\pysiglinewithargsret{\bfcode{encode\_cylinder}}{\emph{sequence}}{}
Return Godel encoding of cylinder set of a sequence (passed as a
list of strings, each one representing a symbol, or a string
in case of a single symbol).

\end{fulllineitems}


\end{fulllineitems}

\index{AbstractGeneralizedShift (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.AbstractGeneralizedShift}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{AbstractGeneralizedShift}}{\emph{alpha\_dod}, \emph{beta\_dod}}{}
The general class.

\end{fulllineitems}

\index{SimpleCFGeneralizedShift (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.SimpleCFGeneralizedShift}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{SimpleCFGeneralizedShift}}{\emph{alpha\_symbols}, \emph{beta\_symbols}, \emph{grammar\_rules}}{}
A simple parser to reproduce example NDA computation in beim
Graben, P., \& Potthast, R. (2014). Universal neural field
computation. In Neural Fields (pp. 299-318).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{alpha\_symbols}} -- list of symbols in the stack alphabet

\item {} 
\textbf{\texttt{beta\_symbols}} -- list of symbols in the input alphabet

\item {} 
\textbf{\texttt{grammar\_rules}} -- 
a dictionary where each entry has
the the form ``X: Y'', where X is a string, and Y is
a list of strings. Each entry corresponds to a rule
in a Context Free Grammar, with X being a Variable
\begin{quote}

and Y being a list of Variables and/or Terminals.
\end{quote}


\end{itemize}

\end{description}\end{quote}
\index{psi() (symdyn.SimpleCFGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.SimpleCFGeneralizedShift.psi}\pysiglinewithargsret{\bfcode{psi}}{\emph{alpha}, \emph{beta}}{}
Apply Generalized Shift phi to dotted sequence.

\end{fulllineitems}

\index{predict() (symdyn.SimpleCFGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.SimpleCFGeneralizedShift.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{alpha}, \emph{beta}}{}
If a rule is present defining how to substitute the symbol in the
alpha DoD, return the new sequence. Otherwise return False (so
that psi can decide to apply attach instead).

\end{fulllineitems}

\index{attach() (symdyn.SimpleCFGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.SimpleCFGeneralizedShift.attach}\pysiglinewithargsret{\bfcode{attach}}{\emph{alpha}, \emph{beta}}{}
If the symbols in the alpha DoD and the beta DoD are equal, pop
them and return the new subsequences. Otherwise just return
the original.

\end{fulllineitems}

\index{lintransf\_params() (symdyn.SimpleCFGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.SimpleCFGeneralizedShift.lintransf_params}\pysiglinewithargsret{\bfcode{lintransf\_params}}{\emph{ge\_alpha}, \emph{ge\_beta}, \emph{alpha}, \emph{beta}}{}
Return two arrays containing respectively the x parameters and the
y parameters of the linear transformation representing the GS
action on the symbologram.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{ge\_alpha}} -- Fractal Encoder for left side of dotted
sequence.

\item {} 
\textbf{\texttt{ge\_beta}} -- Fractal Encoder for right side of dotted
sequence.

\item {} 
\textbf{\texttt{alpha}} -- reversed left side of dotted sequence (as list
of symbols or string representing one symbol).

\item {} 
\textbf{\texttt{beta}} -- right side of dotted sequence (as list of symbols
or string representing one symbol).

\end{itemize}

\item[{Returns}] \leavevmode
{[}\(\lambda_x, a_x\){]}, {[}\(\lambda_y, a_y\){]}

\item[{Return type}] \leavevmode
tuple(numpy.ndarray, numpy.ndarray)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{TMGeneralizedShift (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{TMGeneralizedShift}}{\emph{states}, \emph{tape\_symbols}, \emph{moves}}{}
Class implementing a Generalized Shift simulating a Turing Machine.
For the definition of Generalized Shift, and the characteristics
of GS simulating Turing Machines, see Moore (1991).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{states}} -- list of states.

\item {} 
\textbf{\texttt{tape\_symbols}} -- list of tape symbols.

\item {} 
\textbf{\texttt{moves}} -- a dict of the form (state, symbol): (new state, new
symbol, movement){}`, where movement is equal to either ``L'', ``S'' or
``R'', that is Left, Stay or Right.

\end{itemize}

\end{description}\end{quote}
\index{psi() (symdyn.TMGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift.psi}\pysiglinewithargsret{\bfcode{psi}}{\emph{alpha}, \emph{beta}}{}
Given the right and the inverted left part of a dotted sequence,
apply the generalized shift on them.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{alpha}} -- the inverted left part of a dotted sequence,
as list of symbols.

\item {} 
\textbf{\texttt{beta}} -- the right part of a dotted sequence, as list of symbols.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{F() (symdyn.TMGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift.F}\pysiglinewithargsret{\bfcode{F}}{\emph{alpha}, \emph{beta}}{}
Return direction of shift given simulated TM rule to apply on the
basis of the symbols in the DoD of the dotted sequence.

\end{fulllineitems}

\index{substitution() (symdyn.TMGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift.substitution}\pysiglinewithargsret{\bfcode{substitution}}{\emph{alpha}, \emph{beta}}{}
Return new dotted sequence from substitution.

Substitute symbols in the DoE of the dotted sequence based on the
relevant TM symbol substitution rule, given the symbols in the
dotted sequence DoD.

\end{fulllineitems}

\index{shift() (symdyn.TMGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift.shift}\pysiglinewithargsret{\bfcode{shift}}{\emph{shift\_dir}, \emph{alpha}, \emph{beta}}{}
Shift dotted sequence left or right (shift\_dir = -1 or 1).

\end{fulllineitems}

\index{lintransf\_params() (symdyn.TMGeneralizedShift method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.TMGeneralizedShift.lintransf_params}\pysiglinewithargsret{\bfcode{lintransf\_params}}{\emph{ge\_alpha}, \emph{ge\_beta}, \emph{alpha}, \emph{beta}}{}
Return two arrays containing respectively the x parameters and the
y parameters of the linear transformation representing the GS
action on the symbologram given a dotted sequence.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{ge\_alpha}} -- Fractal Encoder for left side of dotted sequence.

\item {} 
\textbf{\texttt{ge\_beta}} -- Fractal Encoder for right side of dotted sequence.

\item {} 
\textbf{\texttt{alpha}} -- reversed left side of dotted sequence (as list
of symbols).

\item {} 
\textbf{\texttt{beta}} -- right side of dotted sequence (as list of symbols).

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{NonlinearDynamicalAutomaton (class in symdyn)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.NonlinearDynamicalAutomaton}\pysiglinewithargsret{\strong{class }\code{symdyn.}\bfcode{NonlinearDynamicalAutomaton}}{\emph{generalized\_shift}, \emph{godel\_enc\_alpha}, \emph{godel\_enc\_beta}}{}
A Nonlinear Dynamical Automaton from a Generalized Shift and a Godel
Encoding.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{generalized\_shift}} -- an AbstractGeneralizedShift object (as a
base class)

\item {} 
\textbf{\texttt{godel\_enc\_alpha}} -- a GodelEncoder for the \(\alpha
'\) reversed one-side infinite subsequence of the dotted
sequence \(\alpha . \beta\) representing a
configuration of the Turing Machine to be simulated.

\item {} 
\textbf{\texttt{godel\_enc\_beta}} -- 
a GodelEncoder for the \(\beta\)
one-side infinite subsequence of dotted sequence
\(\alpha . \beta\) representing a configuration
\begin{quote}

of the Turing Machine to be simulated.
\end{quote}


\end{itemize}

\end{description}\end{quote}
\index{check\_cell() (symdyn.NonlinearDynamicalAutomaton method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.NonlinearDynamicalAutomaton.check_cell}\pysiglinewithargsret{\bfcode{check\_cell}}{\emph{x}, \emph{y}, \emph{gencoded=False}}{}
Return the coordinates i,j  of the input on the unit square
partition.

If gencoded=True, the input is assumed to be already encoded.

\end{fulllineitems}

\index{find\_flow\_parameters() (symdyn.NonlinearDynamicalAutomaton method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.NonlinearDynamicalAutomaton.find_flow_parameters}\pysiglinewithargsret{\bfcode{find\_flow\_parameters}}{}{}
Convert the generalized shift dynamics in dynamics on the plane,
finding the parameters of the linear transformation for each NDA
cell.

\end{fulllineitems}

\index{flow() (symdyn.NonlinearDynamicalAutomaton method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.NonlinearDynamicalAutomaton.flow}\pysiglinewithargsret{\bfcode{flow}}{\emph{x}, \emph{y}}{}~\begin{description}
\item[{Given \((x_t,y_t)\) return}] \leavevmode
\(\Psi(x_t, y_t) = (x_{t+1}, y_{t+1})\)

\end{description}

\end{fulllineitems}

\index{iterate() (symdyn.NonlinearDynamicalAutomaton method)}

\begin{fulllineitems}
\phantomsection\label{symdyn_docs:symdyn.NonlinearDynamicalAutomaton.iterate}\pysiglinewithargsret{\bfcode{iterate}}{\emph{init\_x}, \emph{init\_y}, \emph{n\_iterations}}{}~\begin{description}
\item[{Apply \(\Psi^n(x_0, y_0)\),}] \leavevmode
where \(x_0\) = init\_x, \(y_0\)
= init\_y, and \(n\) = n\_iterations

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{simpleNNlib module documentation}
\label{simpleNNlib_docs:simplennlib-module-documentation}\label{simpleNNlib_docs::doc}
The simpleNNlib module contains classes to build Neural Networks with saturated-linear and Heaviside activation functions.


\section{Code}
\label{simpleNNlib_docs:code}\label{simpleNNlib_docs:module-simpleNNlib}\index{simpleNNlib (module)}\index{AbstractNNLayer (class in simpleNNlib)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.AbstractNNLayer}\pysiglinewithargsret{\strong{class }\code{simpleNNlib.}\bfcode{AbstractNNLayer}}{\emph{n\_units}, \emph{initial\_values={[}{]}}}{}
The base class for all neuron layers. Each layer has to implement the possibility to
add connections from other layers,
to compute the input to each neuron by summing all contribution from it connections,
and to compute the output given the input and some activation function.

In particular, derived classes will override the activation funcion with their own.
For example, a ramp layer will use a ramp activation function,
a heaviside layer will use a heaviside function, etc...
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{n\_units}} -- the number of units in the layer

\item {} 
\textbf{\texttt{initial\_values}} -- a list of activation values with which the units will be 
initialized

\end{itemize}

\end{description}\end{quote}
\index{sum\_input() (simpleNNlib.AbstractNNLayer method)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.AbstractNNLayer.sum_input}\pysiglinewithargsret{\bfcode{sum\_input}}{}{}
Computes the input contribution from connected layers.

\end{fulllineitems}

\index{activate() (simpleNNlib.AbstractNNLayer method)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.AbstractNNLayer.activate}\pysiglinewithargsret{\bfcode{activate}}{}{}
Computes the units activation.

\end{fulllineitems}


\end{fulllineitems}

\index{HeavisideLayer (class in simpleNNlib)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.HeavisideLayer}\pysiglinewithargsret{\strong{class }\code{simpleNNlib.}\bfcode{HeavisideLayer}}{\emph{n\_units}, \emph{centers}, \emph{inclusive={[}{]}}}{}
Class implementing a layer of neurons with Heaviside activation function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{n\_units}} -- number of units in layer.

\item {} 
\textbf{\texttt{centers}} -- specifies \(c_i\) for each unit \(i\) , given the unit activation function \(H_i(x-c_i)\) .
Equivalent to adding a bias of \(-c_i\) .

\item {} 
\textbf{\texttt{inclusive}} -- if \(H_i(-c_i)=1\), the \(i\)-th item in the list is True, 
otherwise it's False and \(H_i(-c_i)=0\).

\end{itemize}

\end{description}\end{quote}
\index{activate() (simpleNNlib.HeavisideLayer method)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.HeavisideLayer.activate}\pysiglinewithargsret{\bfcode{activate}}{}{}
Computes the units activation

\end{fulllineitems}


\end{fulllineitems}

\index{RampLayer (class in simpleNNlib)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.RampLayer}\pysiglinewithargsret{\strong{class }\code{simpleNNlib.}\bfcode{RampLayer}}{\emph{n\_units}, \emph{biases=0}, \emph{initial\_values={[}{]}}}{}
Class implementing a layer of neurons with saturated linear activation function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{n\_units}} -- number of units in layer.

\item {} 
\textbf{\texttt{biases}} -- a list specifying the bias for each neuron.

\item {} 
\textbf{\texttt{initial\_values}} -- initial activation values

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{LayerSlice (class in simpleNNlib)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.LayerSlice}\pysiglinewithargsret{\strong{class }\code{simpleNNlib.}\bfcode{LayerSlice}}{\emph{layer}, \emph{a\_slice}}{}
This class is used so that to access the activation for a given layer \emph{nnlayer},
you can just use the notation \emph{nnlayer{[}someslice{]}}, where \emph{someslice} is the usual Python slice.

\end{fulllineitems}

\index{Connection (class in simpleNNlib)}

\begin{fulllineitems}
\phantomsection\label{simpleNNlib_docs:simpleNNlib.Connection}\pysiglinewithargsret{\strong{class }\code{simpleNNlib.}\bfcode{Connection}}{\emph{from\_layer}, \emph{to\_layer}, \emph{connection\_matrix}}{}
Class implementing a connection. Basically used to keep track of who is connected to who at this moment.
Also, you can modify the connection by modifying conn\_mat.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{from\_layer}} -- layer from which the connection is established.

\item {} 
\textbf{\texttt{to\_layer}} -- layer to which the connection is established.

\item {} 
\textbf{\texttt{connection\_matrix}} -- matrix specifying connections and weights.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\chapter{neuraltm module documentation}
\label{neuraltm_docs:neuraltm-module-documentation}\label{neuraltm_docs::doc}
The module NeuralTM contains classes to build an NDA Neural Network simulating a Turing Machine in real time.


\section{Code}
\label{neuraltm_docs:code}\label{neuraltm_docs:module-neuraltm}\index{neuraltm (module)}\index{NeuralTM (class in neuraltm)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM}\pysiglinewithargsret{\strong{class }\code{neuraltm.}\bfcode{NeuralTM}}{\emph{nda}, \emph{cylinder\_sets=False}}{}
Given an NDA, constructs the equivalent neural network described in
our submitted paper.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{nda}} -- a symdyn.NonlinearDynamicalAutomaton object

\end{description}\end{quote}
\index{cn\_BSLbx\_LTL() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_BSLbx_LTL}\pysiglinewithargsret{\bfcode{cn\_BSLbx\_LTL}}{}{}
Generate the connection matrix between the x cell selection layer
and the linear transformation layer.  The connection pattern
is discussed in our submitted paper.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between x cell selection layer and

\end{description}\end{quote}

linear transformation layer
\begin{quote}\begin{description}
\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{cn\_BSLby\_LTL() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_BSLby_LTL}\pysiglinewithargsret{\bfcode{cn\_BSLby\_LTL}}{}{}
Generates the connection matrix between the y cell selection layer
and the linear transformation layer.  The connection pattern
is discussed in our submitted paper.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between y cell selection layer and

\end{description}\end{quote}

linear transformation layer
\begin{quote}\begin{description}
\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{cn\_MCLx\_LTL() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_MCLx_LTL}\pysiglinewithargsret{\bfcode{cn\_MCLx\_LTL}}{}{}
Generates the connection matrix between the x input neurons and
the linear transformation layer.  These are the connections
implementing the linear transformations of the NDA piecewise
linear system.  The multiplication constants are implemented
as multiplicative weights in the connection matrix, whereas
the additive constants are implemented as biases to the linear
transformation layer, outside this function.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between x input neurons and linear
transformation layer

\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{cn\_MCLy\_LTL() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_MCLy_LTL}\pysiglinewithargsret{\bfcode{cn\_MCLy\_LTL}}{}{}
Generates the connection matrix between the y input neurons and
the linear transformation layer.  These are the connections
implementing the linear transformations of the NDA piecewise
linear system.  The multiplication constants are implemented
as multiplicative weights in the connection matrix, whereas
the additive constants are implemented as biases to the linear
transformation layer, outside this function.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between y input neurons and linear

\end{description}\end{quote}

transformation layer
\begin{quote}\begin{description}
\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{LTL\_biases\_from\_params() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.LTL_biases_from_params}\pysiglinewithargsret{\bfcode{LTL\_biases\_from\_params}}{}{}
Generates the biases for the neurons in the linear transformation
layer, which implement the additive constants of the linear
transfomations of the NDA piecewise linear system.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
biases for linear transformation layer neurons

\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{cn\_LTL\_MCLx() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_LTL_MCLx}\pysiglinewithargsret{\bfcode{cn\_LTL\_MCLx}}{}{}~\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between linear transformation layer and

\end{description}\end{quote}

x input neurons.
\begin{quote}\begin{description}
\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{cn\_LTL\_MCLy() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.cn_LTL_MCLy}\pysiglinewithargsret{\bfcode{cn\_LTL\_MCLy}}{}{}~\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
connection matrix between linear transformation layer and

\end{description}\end{quote}

x input neurons.
\begin{quote}\begin{description}
\item[{Return type}] \leavevmode
numpy ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{run\_net() (neuraltm.NeuralTM method)}

\begin{fulllineitems}
\phantomsection\label{neuraltm_docs:neuraltm.NeuralTM.run_net}\pysiglinewithargsret{\bfcode{run\_net}}{\emph{init\_x=None}, \emph{init\_y=None}, \emph{n\_iterations=1}}{}
Run the network, given the initial values for the x and y input
neurons and the number of iterations
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{init\_x}} -- initial activation values for left and right x

\end{description}\end{quote}

input neurons
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{init\_y}} -- initial activation values for left and right y

\end{description}\end{quote}

input neurons
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{n\_iterations}} -- the desired number of network iterations

\item[{Returns}] \leavevmode
list containing the input activation for each
iteration. Each tuple item in the list contains two
arrays, respectively containing the activations for the x
and the y input neurons.  \code{{[} (np.array({[}x\_l\_act0,
x\_r\_act0{]}), np.array({[}y\_l\_act0, y\_r\_act0{]})), ... {]}}

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\begin{thebibliography}{Moore91}
\bibitem[Moore91]{Moore91}{\phantomsection\label{symdyn_docs:moore91} 
Moore, C. (1991). Generalized shifts: unpredictability and undecidability in dynamical systems. Nonlinearity, 4(2), 199.
}
\end{thebibliography}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{n}
\item {\texttt{neuraltm}}, \pageref{neuraltm_docs:module-neuraltm}
\indexspace
\bigletter{s}
\item {\texttt{simpleNNlib}}, \pageref{simpleNNlib_docs:module-simpleNNlib}
\item {\texttt{symdyn}}, \pageref{symdyn_docs:module-symdyn}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
